<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>TensorFlow-安装与使用 | Henry Fox</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="简介记录一下学习Tensorflow的过程，包括代码和教程。 学习资料包括http:&#x2F;&#x2F;wiki.jikexueyuan.com&#x2F;project&#x2F;tensorflow-zh&#x2F;tutorials https:&#x2F;&#x2F;www.tensorflow.org&#x2F;get_started&#x2F; 代码见https:&#x2F;&#x2F;github.com&#x2F;fengh16&#x2F;TensorFlow-Learning 一些公式使用$$标记，">
<meta property="og:type" content="article">
<meta property="og:title" content="TensorFlow-安装与使用">
<meta property="og:url" content="https://fengh16.github.io/2018/04/19/TensorFlow-%E5%AE%89%E8%A3%85%E4%B8%8E%E4%BD%BF%E7%94%A8/index.html">
<meta property="og:site_name" content="Henry Fox">
<meta property="og:description" content="简介记录一下学习Tensorflow的过程，包括代码和教程。 学习资料包括http:&#x2F;&#x2F;wiki.jikexueyuan.com&#x2F;project&#x2F;tensorflow-zh&#x2F;tutorials https:&#x2F;&#x2F;www.tensorflow.org&#x2F;get_started&#x2F; 代码见https:&#x2F;&#x2F;github.com&#x2F;fengh16&#x2F;TensorFlow-Learning 一些公式使用$$标记，">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/5060560-35b3c54ea13427c5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/5060560-58926e593ddedcb3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/5060560-a32ddfe22c0c22a2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/5060560-dc79b750808ee929.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/5060560-088aa36d659ecf1a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/5060560-ac8a0f00b9f836b0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="article:published_time" content="2018-04-18T22:06:06.000Z">
<meta property="article:modified_time" content="2021-07-02T06:50:14.174Z">
<meta property="article:author" content="Henry Fox">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://upload-images.jianshu.io/upload_images/5060560-35b3c54ea13427c5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
  
    <link rel="alternate" href="/atom.xml" title="Henry Fox" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
<meta name="generator" content="Hexo 5.4.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Henry Fox</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://fengh16.github.io"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-TensorFlow-安装与使用" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2018/04/19/TensorFlow-%E5%AE%89%E8%A3%85%E4%B8%8E%E4%BD%BF%E7%94%A8/" class="article-date">
  <time class="dt-published" datetime="2018-04-18T22:06:06.000Z" itemprop="datePublished">2018-04-19</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E4%BD%BF%E7%94%A8%E2%80%94%E8%BD%AF%E4%BB%B6%E2%80%94TF/">使用—软件—TF</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      TensorFlow-安装与使用
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <meta name="referrer" content="no-referrer" />

<h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>记录一下学习Tensorflow的过程，包括代码和教程。</p>
<p>学习资料包括<a target="_blank" rel="noopener" href="http://wiki.jikexueyuan.com/project/tensorflow-zh/tutorials">http://wiki.jikexueyuan.com/project/tensorflow-zh/tutorials</a></p>
<p><a target="_blank" rel="noopener" href="https://www.tensorflow.org/get_started/">https://www.tensorflow.org/get_started/</a></p>
<p>代码见<a target="_blank" rel="noopener" href="https://github.com/fengh16/TensorFlow-Learning">https://github.com/fengh16/TensorFlow-Learning</a></p>
<p>一些公式使用$$标记，如果显示不出请使用typora并且复制进去读取。</p>
<h1 id="环境"><a href="#环境" class="headerlink" title="环境"></a>环境</h1><p>Windows 10,Ubuntu18.04 GTX 950M； （Ubuntu安装过程见<a target="_blank" rel="noopener" href="https://www.jianshu.com/p/a4d72364a7ee">https://www.jianshu.com/p/a4d72364a7ee</a> 相关部分）</p>
<p>腾讯云辣鸡学生机（ubuntu 1U2G）……反正只放了一个公众号后台，跑跑tensorflow也无妨，大不了跑个几天几夜</p>
<h1 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h1><p>安装python3, pip等，之后ubuntu系统直接<code>pip install tensorflow</code>，win10下面……一定要下载<strong>64位</strong>的python 3.5/3.6（我也不知道为啥原来下载的是32位的）！之后直接<code>pip install tensorflow</code>。如果有报错，试试<code>pip install --upgrade --ignore-installed tensorflow</code></p>
<p>（记得换源加速，见<a target="_blank" rel="noopener" href="https://www.jianshu.com/p/a8b84ac0e609%EF%BC%89">https://www.jianshu.com/p/a8b84ac0e609）</a></p>
<p>如果安装gpu版本：</p>
<ul>
<li><p>因为官方的是基于cuda 9.0版本的，所以就直接安装cuda9.0版本，之后安装cudnn并且复制文件就行。</p>
<ul>
<li>复制一堆文件<ul>
<li>Copy \cuda\bin\cudnn64_7.dll to C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.0\bin.</li>
<li>Copy \cuda\ include\cudnn.h to C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.0\include.</li>
<li>Copy \cuda\lib\x64\cudnn.lib to C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.0\lib\x64.</li>
</ul>
</li>
</ul>
</li>
<li><p>如果想用9.1版本的……那就折腾吧……（原始教程：<a target="_blank" rel="noopener" href="https://blog.csdn.net/vcvycy/article/details/79298703%EF%BC%89">https://blog.csdn.net/vcvycy/article/details/79298703）</a></p>
<ul>
<li>安装cuda 9.1+VS2017</li>
<li>安装cudnn7.0</li>
<li>复制一堆文件<ul>
<li>Copy \cuda\bin\cudnn64_7.dll to C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.0\bin.</li>
<li>Copy \cuda\ include\cudnn.h to C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.0\include.</li>
<li>Copy \cuda\lib\x64\cudnn.lib to C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.0\lib\x64.</li>
</ul>
</li>
<li>到<a target="_blank" rel="noopener" href="https://github.com/fo40225/tensorflow-windows-wheel%E4%B8%8B%E8%BD%BDCuda9.1%E7%89%88%E6%9C%AC%E7%9A%84tensorflow">https://github.com/fo40225/tensorflow-windows-wheel下载Cuda9.1版本的tensorflow</a></li>
</ul>
</li>
</ul>
<h1 id="检测TensorFlow安装"><a href="#检测TensorFlow安装" class="headerlink" title="检测TensorFlow安装"></a>检测TensorFlow安装</h1><p>代码见<a target="_blank" rel="noopener" href="https://github.com/fengh16/TensorFlow-Learning/blob/master/001-FirstTry-180419/test.py">https://github.com/fengh16/TensorFlow-Learning/blob/master/001-FirstTry-180419/test.py</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> absolute_import, division, print_function</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> tensorflow.contrib.eager <span class="keyword">as</span> tfe</span><br><span class="line"></span><br><span class="line">tf.enable_eager_execution()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;TensorFlow version: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(tf.VERSION))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Eager execution: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(tf.executing_eagerly()))</span><br></pre></td></tr></table></figure>

<h1 id="第一个程序"><a href="#第一个程序" class="headerlink" title="第一个程序"></a>第一个程序</h1><p>代码见<a target="_blank" rel="noopener" href="https://github.com/fengh16/TensorFlow-Learning/blob/master/001-FirstTry-180419/try.py">https://github.com/fengh16/TensorFlow-Learning/blob/master/001-FirstTry-180419/try.py</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 生成三维数据之后用平面拟合</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用 NumPy 生成假数据(phony data), 总共 100 个点.</span></span><br><span class="line">x_data = np.float32(np.random.rand(<span class="number">2</span>, <span class="number">100</span>)) <span class="comment"># 随机输入</span></span><br><span class="line">y_data = np.dot([<span class="number">0.100</span>, <span class="number">0.200</span>], x_data) + <span class="number">0.300</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 构造一个线性模型</span></span><br><span class="line">b = tf.Variable(tf.zeros([<span class="number">1</span>]))</span><br><span class="line">W = tf.Variable(tf.random_uniform([<span class="number">1</span>, <span class="number">2</span>], -<span class="number">1.0</span>, <span class="number">1.0</span>))</span><br><span class="line">y = tf.matmul(W, x_data) + b</span><br><span class="line"></span><br><span class="line"><span class="comment"># 最小化方差</span></span><br><span class="line">loss = tf.reduce_mean(tf.square(y - y_data))</span><br><span class="line">optimizer = tf.train.GradientDescentOptimizer(<span class="number">0.5</span>)</span><br><span class="line">train = optimizer.minimize(loss)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化变量</span></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动图 (graph)</span></span><br><span class="line">sess = tf.Session()</span><br><span class="line">sess.run(init)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 拟合平面</span></span><br><span class="line"><span class="keyword">for</span> step <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">201</span>):</span><br><span class="line">    sess.run(train)</span><br><span class="line">    <span class="keyword">if</span> step % <span class="number">20</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(step, sess.run(W), sess.run(b))</span><br></pre></td></tr></table></figure>

<p>运行后会输出结果。</p>
<h1 id="MNIST机器学习入门"><a href="#MNIST机器学习入门" class="headerlink" title="MNIST机器学习入门"></a>MNIST机器学习入门</h1><p>MNIST是一个入门级的计算机视觉数据集，包含各种手写图片以及标签。</p>
<h2 id="下载数据集"><a href="#下载数据集" class="headerlink" title="下载数据集"></a>下载数据集</h2><p>直接下载<a target="_blank" rel="noopener" href="http://yann.lecun.com/exdb/mnist/%E6%88%96%E8%80%85https://github.com/fengh16/TensorFlow-Learning/tree/master/002-MNIST-180420/MNIST_data%E9%87%8C%E9%9D%A2%E7%9A%84%E5%9B%9B%E4%B8%AA%E5%8E%8B%E7%BC%A9%E5%8C%85%EF%BC%8C%E7%84%B6%E5%90%8E%E5%A4%8D%E5%88%B6%E5%88%B0%E9%9C%80%E8%A6%81%E7%9A%84%E8%B7%AF%E5%BE%84%E4%B8%8B%EF%BC%88%E5%A6%82%60MNIST_data%60%E6%96%87%E4%BB%B6%E5%A4%B9%E4%B8%8B%EF%BC%89">http://yann.lecun.com/exdb/mnist/或者https://github.com/fengh16/TensorFlow-Learning/tree/master/002-MNIST-180420/MNIST_data里面的四个压缩包，然后复制到需要的路径下（如`MNIST_data`文件夹下）</a></p>
<p>或者下载运行<a target="_blank" rel="noopener" href="https://github.com/fengh16/TensorFlow-Learning/tree/master/002-MNIST-180420/download.py">https://github.com/fengh16/TensorFlow-Learning/tree/master/002-MNIST-180420/download.py</a></p>
<h2 id="数据集介绍"><a href="#数据集介绍" class="headerlink" title="数据集介绍"></a>数据集介绍</h2><p>下载下来的数据集被分成两部分：60000行的训练数据集（<code>mnist.train</code>）和10000行的测试数据集（<code>mnist.test</code>）。</p>
<p>每一个MNIST数据单元有两部分组成：一张包含手写数字的图片和一个对应的标签。我们把这些图片设为“xs”，把这些标签设为“ys”。训练数据集和测试数据集都包含xs和ys，比如训练数据集的图片是 <code>mnist.train.images</code> ，训练数据集的标签是 <code>mnist.train.labels</code>。</p>
<p>每一张图片包含28X28个像素点。我们可以用一个数字数组来表示这张图片。我们把这个数组展开成一个向量，长度是 28x28 = 784。如何展开这个数组（数字间的顺序）不重要，只要保持各个图片采用相同的方式展开。从这个角度来看，MNIST数据集的图片就是在784维向量空间里面的点。因此，在MNIST训练数据集中，<code>mnist.train.images</code> 是一个形状为 [60000, 784] 的张量，第一个维度数字用来索引图片，第二个维度数字用来索引每张图片中的像素点。在此张量里的每一个元素，都表示某张图片里的某个像素的强度值，值介于0和1之间。相对应的MNIST数据集的标签是介于0到9的数字，用来描述给定图片里表示的数字。我们使标签数据是”<code>one-hot vectors</code>“。 一个<code>one-hot</code>向量除了某一位的数字是1以外其余各维度数字都是0。所以在此教程中，数字n将表示成一个只有在第n维度（从0开始）数字为1的10维向量。比如，标签0将表示成([1,0,0,0,0,0,0,0,0,0,0])。因此， <code>mnist.train.labels</code> 是一个 [60000, 10] 的数字矩阵。</p>
<h2 id="Softmax回归介绍"><a href="#Softmax回归介绍" class="headerlink" title="Softmax回归介绍"></a>Softmax回归介绍</h2><p><code>softmax模型</code>可以用来给不同的对象分配概率。即使在之后，我们训练更加精细的模型时，最后一步也需要用<code>softmax</code>来分配概率。</p>
<p><code>softmax回归</code>（<code>softmax regression</code>）：</p>
<ul>
<li>为了得到一张给定图片属于某个特定数字类的证据（<code>evidence</code>），我们对图片像素值进行<strong>加权求和</strong>。如果这个像素具有很强的证据说明这张图片不属于该类，那么相应的权值为负数，相反如果这个像素拥有有利的证据支持这张图片属于这个类，那么权值是正数。</li>
<li>下面的图片显示了一个模型学习到的图片上每个像素对于特定数字类的权值。红色代表负数权值，蓝色代表正数权值。<br><img src="https://upload-images.jianshu.io/upload_images/5060560-35b3c54ea13427c5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="权值示意"></li>
<li>我们也需要加入一个额外的偏置量（bias），因为输入往往会带有一些无关的干扰量。因此对于给定的输入图片 x 它代表的是数字 i 的证据可以表示为：$evidence_i = \sum\limits_j W_{i, j}x_j + b_j$ ，其中$W_{i,j}$表示输入图片像素位置为$j$的地方是$i$的权重，$b_i$表示数字i的偏置量，$j$代表给定图片x的像素引用于像素求和。之后用softmax函数可以把这些证据转换成概率 y：$y=softmax(evidence)$</li>
<li>这里的softmax可以看成是一个激励（activation）函数或者链接（link）函数，把我们定义的线性函数的输出转换成我们想要的格式，也就是关于10个数字类的概率分布。因此，给定一张图片，它对于每一个数字的吻合度可以被softmax函数转换成为一个概率值。softmax函数可以使用正态分布的方法$normalize(exp(x))$</li>
<li>这个回归模型可以用以下图解释：<br><img src="https://upload-images.jianshu.io/upload_images/5060560-58926e593ddedcb3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="计算"></li>
</ul>
<h2 id="python实现"><a href="#python实现" class="headerlink" title="python实现"></a>python实现</h2><p>为了用python实现高效的数值计算，我们通常会使用函数库，比如NumPy，会把类似矩阵乘法这样的复杂运算使用其他外部语言实现。不幸的是，从外部计算切换回Python的每一个操作，仍然是一个很大的开销。如果你用GPU来进行外部计算，这样的开销会更大。用分布式的计算方式，也会花费更多的资源用来传输数据。</p>
<p>TensorFlow也把复杂的计算放在python之外完成，但是为了避免前面说的那些开销，它做了进一步完善。Tensorflow不单独地运行单一的复杂计算，而是让我们可以先用图描述一系列可交互的计算操作，然后全部一起在Python之外运行。（这样类似的运行方式，可以在不少的机器学习库中看到。）</p>
<p>代码见 <a target="_blank" rel="noopener" href="https://github.com/fengh16/TensorFlow-Learning/blob/master/002-MNIST-180420/mnist.py">github</a>或者<a target="_blank" rel="noopener" href="https://github.com/fengh16/TensorFlow-Learning/blob/master/002-MNIST-180420/mnist_.py">注释简洁的</a></p>
<p>使用TensorFlow之前，首先导入它：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br></pre></td></tr></table></figure>

<p>我们通过操作符号变量来描述这些可交互的操作单元，可以用下面的方式创建一个：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x &#x3D; tf.placeholder(tf.float32, [None, 784])</span><br></pre></td></tr></table></figure>

<p><code>x</code>不是一个特定的值，而是一个占位符<code>placeholder</code>，我们在TensorFlow运行计算时输入这个值。我们希望能够输入任意数量的MNIST图像，每一张图展平成784维的向量。我们用2维的浮点数张量来表示这些图，这个张量的形状是<code>[None，784 ]</code>。（这里的<code>None</code>表示此张量的第一个维度可以是<strong>任何长度</strong>的。）</p>
<p>我们的模型也需要权重值和偏置量，当然我们可以把它们当做是另外的输入（使用占位符），但TensorFlow有一个更好的方法来表示它们：<code>Variable</code> 。 一个<code>Variable</code>代表一个可修改的张量，存在在TensorFlow的用于描述交互性操作的图中。它们可以用于计算输入值，也可以在计算中被修改。对于各种机器学习应用，一般都会有模型参数，可以用<code>Variable</code>表示。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">W &#x3D; tf.Variable(tf.zeros([784,10]))</span><br><span class="line">b &#x3D; tf.Variable(tf.zeros([10]))</span><br></pre></td></tr></table></figure>

<p>我们赋予<code>tf.Variable</code>不同的初值来创建不同的<code>Variable</code>：在这里，我们都用全为零的张量来初始化<code>W</code>和<code>b</code>。因为我们要学习<code>W</code>和<code>b</code>的值，它们的初值可以随意设置。</p>
<p>注意，<code>W</code>的维度是[784，10]，因为我们想要用784维的图片向量乘以它以得到一个10维的证据值向量，每一位对应不同数字类。<code>b</code>的形状是[10]，所以我们可以直接把它加到输出上面。</p>
<p>现在，我们可以实现我们的模型啦。只需要一行代码！</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y &#x3D; tf.nn.softmax(tf.matmul(x,W) + b)</span><br></pre></td></tr></table></figure>

<p>首先，我们用<code>tf.matmul(X，W)</code>表示<code>x</code>乘以<code>W</code>，对应之前等式里面的<img src="http://upload-images.jianshu.io/upload_images/5060560-a32ddfe22c0c22a2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"></p>
<p>，这里<code>x</code>是一个2维张量拥有多个输入。然后再加上<code>b</code>，把和输入到<code>tf.nn.softmax</code>函数里面。</p>
<p>至此，我们先用了几行简短的代码来设置变量，然后只用了一行代码来定义我们的模型。TensorFlow不仅仅可以使softmax回归模型计算变得特别简单，它也用这种非常灵活的方式来描述其他各种数值计算，从机器学习模型对物理学模拟仿真模型。一旦被定义好之后，我们的模型就可以在不同的设备上运行：计算机的CPU，GPU，甚至是手机！</p>
<h2 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h2><p>为了训练我们的模型，我们首先需要定义一个指标来评估这个模型是好的。其实，在机器学习，我们通常定义指标来表示一个模型是坏的，这个指标称为成本（cost）或损失（loss），然后尽量最小化这个指标。但是，这两种方式是相同的。</p>
<p>一个非常常见的，非常漂亮的成本函数是“交叉熵”（cross-entropy）。交叉熵产生于信息论里面的信息压缩编码技术，但是它后来演变成为从博弈论到机器学习等其他领域里的重要技术手段。它的定义如下：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/5060560-dc79b750808ee929.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"></p>
<p><strong>y</strong> 是我们预测的概率分布, <strong>y’</strong> 是实际的分布（我们输入的<code>one-hot vector</code>)。比较粗糙的理解是，交叉熵是用来衡量我们的预测用于描述真相的低效性。更详细的关于交叉熵的解释超出本教程的范畴，但是你很有必要好好<a target="_blank" rel="noopener" href="http://colah.github.io/posts/2015-09-Visual-Information/">理解它</a>。</p>
<p>为了计算交叉熵，我们首先需要添加一个新的占位符用于输入正确值：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y_ &#x3D; tf.placeholder(&quot;float&quot;, [None,10])</span><br></pre></td></tr></table></figure>

<p>然后我们可以用<img src="http://upload-images.jianshu.io/upload_images/5060560-088aa36d659ecf1a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"></p>
<p>计算交叉熵:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cross_entropy &#x3D; -tf.reduce_sum(y_*tf.log(y))</span><br></pre></td></tr></table></figure>

<p>首先，用 <code>tf.log</code> 计算 <code>y</code> 的每个元素的对数。接下来，我们把 <code>y_</code> 的每一个元素和 <code>tf.log(y)</code> 的对应元素相乘。最后，用 <code>tf.reduce_sum</code> 计算张量的所有元素的总和。（注意，这里的交叉熵不仅仅用来衡量单一的一对预测和真实值，而是所有100幅图片的交叉熵的总和。对于100个数据点的预测表现比单一数据点的表现能更好地描述我们的模型的性能。</p>
<p>现在我们知道我们需要我们的模型做什么啦，用TensorFlow来训练它是非常容易的。因为TensorFlow拥有一张描述你各个计算单元的图，它可以自动地使用<a target="_blank" rel="noopener" href="http://colah.github.io/posts/2015-08-Backprop/">反向传播算法(backpropagation algorithm)</a>来有效地确定你的变量是如何影响你想要最小化的那个成本值的。然后，TensorFlow会用你选择的优化算法来不断地修改变量以降低成本。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_step &#x3D; tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)</span><br></pre></td></tr></table></figure>

<p>在这里，我们要求TensorFlow用梯度下降算法（gradient descent algorithm）以0.01的学习速率最小化交叉熵。梯度下降算法（gradient descent algorithm）是一个简单的学习过程，TensorFlow只需将每个变量一点点地往使成本不断降低的方向移动。当然TensorFlow也提供了<a target="_blank" rel="noopener" href="http://wiki.jikexueyuan.com/project/tensorflow-zh/api_docs/python/train.html">其他许多优化算法</a>：只要简单地调整一行代码就可以使用其他的算法。</p>
<p>TensorFlow在这里实际上所做的是，它会在后台给描述你的计算的那张图里面增加一系列新的计算操作单元用于实现反向传播算法和梯度下降算法。然后，它返回给你的只是一个单一的操作，当运行这个操作时，它用梯度下降算法训练你的模型，微调你的变量，不断减少成本。</p>
<p>现在，我们已经设置好了我们的模型。在运行计算之前，我们需要添加一个操作来初始化我们创建的变量：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">init &#x3D; tf.initialize_all_variables()</span><br></pre></td></tr></table></figure>

<p>现在我们可以在一个<code>Session</code>里面启动我们的模型，并且初始化变量：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sess &#x3D; tf.Session()</span><br><span class="line">sess.run(init)</span><br></pre></td></tr></table></figure>

<p>然后开始训练模型，这里我们让模型循环训练1000次！</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">for i in range(1000):</span><br><span class="line">  batch_xs, batch_ys &#x3D; mnist.train.next_batch(100)</span><br><span class="line">  sess.run(train_step, feed_dict&#x3D;&#123;x: batch_xs, y_: batch_ys&#125;)</span><br></pre></td></tr></table></figure>

<p>该循环的每个步骤中，我们都会随机抓取训练数据中的100个批处理数据点，然后我们用这些数据点作为参数替换之前的占位符来运行<code>train_step</code>。</p>
<p>使用一小部分的随机数据来进行训练被称为随机训练（stochastic training）- 在这里更确切的说是随机梯度下降训练。在理想情况下，我们希望用我们所有的数据来进行每一步的训练，因为这能给我们更好的训练结果，但显然这需要很大的计算开销。所以，每一次训练我们可以使用不同的数据子集，这样做既可以减少计算开销，又可以最大化地学习到数据集的总体特性。</p>
<h2 id="评估我们的模型"><a href="#评估我们的模型" class="headerlink" title="评估我们的模型"></a>评估我们的模型</h2><p>那么我们的模型性能如何呢？</p>
<p>首先让我们找出那些预测正确的标签。<code>tf.argmax</code> 是一个非常有用的函数，它能给出某个tensor对象在某一维上的其数据最大值所在的索引值。由于标签向量是由0,1组成，因此最大值1所在的索引位置就是类别标签，比如<code>tf.argmax(y,1)</code>返回的是模型对于任一输入x预测到的标签值，而 <code>tf.argmax(y_,1)</code> 代表正确的标签，我们可以用 <code>tf.equal</code> 来检测我们的预测是否真实标签匹配(索引位置一样表示匹配)。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">correct_prediction &#x3D; tf.equal(tf.argmax(y,1), tf.argmax(y_,1))</span><br></pre></td></tr></table></figure>

<p>这行代码会给我们一组布尔值。为了确定正确预测项的比例，我们可以把布尔值转换成浮点数，然后取平均值。例如，<code>[True, False, True, True]</code> 会变成 <code>[1,0,1,1]</code> ，取平均值后得到 <code>0.75</code>.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">accuracy &#x3D; tf.reduce_mean(tf.cast(correct_prediction, &quot;float&quot;))</span><br></pre></td></tr></table></figure>

<p>最后，我们计算所学习到的模型在测试数据集上面的正确率。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print sess.run(accuracy, feed_dict&#x3D;&#123;x: mnist.test.images, y_: mnist.test.labels&#125;)</span><br></pre></td></tr></table></figure>

<p>这个最终结果值应该大约是91%。</p>
<h2 id="MNIST优化"><a href="#MNIST优化" class="headerlink" title="MNIST优化"></a>MNIST优化</h2><p>代码见<a target="_blank" rel="noopener" href="https://github.com/fengh16/TensorFlow-Learning/blob/master/002-MNIST-180420/mnist-advanced.py">Github</a></p>
<h3 id="权重初始化："><a href="#权重初始化：" class="headerlink" title="权重初始化："></a>权重初始化：</h3><p>为了创建这个模型，我们需要创建大量的权重和偏置项。这个模型中的权重在初始化时应该加入少量的噪声来打破对称性以及避免0梯度。由于我们使用的是ReLU神经元，因此比较好的做法是用一个较小的正数来初始化偏置项，以避免神经元节点输出恒为0的问题（dead neurons）。为了不在建立模型的时候反复做初始化操作，我们定义两个函数用于初始化。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">def weight_variable(shape):</span><br><span class="line">  initial &#x3D; tf.truncated_normal(shape, stddev&#x3D;0.1)</span><br><span class="line">  return tf.Variable(initial)</span><br><span class="line"></span><br><span class="line">def bias_variable(shape):</span><br><span class="line">  initial &#x3D; tf.constant(0.1, shape&#x3D;shape)</span><br><span class="line">  return tf.Variable(initial)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="卷积和池化"><a href="#卷积和池化" class="headerlink" title="卷积和池化"></a><a target="_blank" rel="noopener" href="https://www.jianshu.com/p/ffca7c28e9cc">卷积</a>和池化</h3><p>TensorFlow在卷积和池化上有很强的灵活性。我们怎么处理边界？步长应该设多大？在这个实例里，我们会一直使用vanilla版本。我们的卷积使用1步长（stride size），0边距（padding size）的模板，保证输出和输入是同一个大小。我们的池化用简单传统的2x2大小的模板做max pooling。为了代码更简洁，我们把这部分抽象成一个函数。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">def conv2d(x, W):</span><br><span class="line">  return tf.nn.conv2d(x, W, strides&#x3D;[1, 1, 1, 1], padding&#x3D;&#39;SAME&#39;)</span><br><span class="line"></span><br><span class="line">def max_pool_2x2(x):</span><br><span class="line">  return tf.nn.max_pool(x, ksize&#x3D;[1, 2, 2, 1],</span><br><span class="line">                        strides&#x3D;[1, 2, 2, 1], padding&#x3D;&#39;SAME&#39;)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="第一层卷积"><a href="#第一层卷积" class="headerlink" title="第一层卷积"></a>第一层卷积</h4><p>现在我们可以开始实现第一层了。它由一个卷积接一个max pooling完成。卷积在每个5x5的patch中算出32个特征。卷积的权重张量形状是<code>[5, 5, 1, 32]</code>，前两个维度是patch的大小，接着是输入的通道数目，最后是输出的通道数目。 而对于每一个输出通道都有一个对应的偏置量。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">W_conv1 &#x3D; weight_variable([5, 5, 1, 32])</span><br><span class="line">b_conv1 &#x3D; bias_variable([32])</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>为了用这一层，我们把<code>x</code>变成一个4d向量，其第2、第3维对应图片的宽、高，最后一维代表图片的颜色通道数(因为是灰度图所以这里的通道数为1，如果是rgb彩色图，则为3)。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x_image &#x3D; tf.reshape(x, [-1,28,28,1])</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>We then convolve <code>x_image</code> with the weight tensor, add the bias, apply the ReLU function, and finally max pool. 我们把<code>x_image</code>和权值向量进行卷积，加上偏置项，然后应用ReLU激活函数，最后进行max pooling。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">h_conv1 &#x3D; tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)</span><br><span class="line">h_pool1 &#x3D; max_pool_2x2(h_conv1)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="第二层卷积"><a href="#第二层卷积" class="headerlink" title="第二层卷积"></a>第二层卷积</h4><p>为了构建一个更深的网络，我们会把几个类似的层堆叠起来。第二层中，每个5x5的patch会得到64个特征。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">W_conv2 &#x3D; weight_variable([5, 5, 32, 64])</span><br><span class="line">b_conv2 &#x3D; bias_variable([64])</span><br><span class="line"></span><br><span class="line">h_conv2 &#x3D; tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)</span><br><span class="line">h_pool2 &#x3D; max_pool_2x2(h_conv2)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="密集连接层"><a href="#密集连接层" class="headerlink" title="密集连接层"></a>密集连接层</h4><p>现在，图片尺寸减小到7x7，我们加入一个有1024个神经元的全连接层，用于处理整个图片。我们把池化层输出的张量reshape成一些向量，乘上权重矩阵，加上偏置，然后对其使用ReLU。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">W_fc1 &#x3D; weight_variable([7 * 7 * 64, 1024])</span><br><span class="line">b_fc1 &#x3D; bias_variable([1024])</span><br><span class="line"></span><br><span class="line">h_pool2_flat &#x3D; tf.reshape(h_pool2, [-1, 7*7*64])</span><br><span class="line">h_fc1 &#x3D; tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h5 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h5><p>为了减少过拟合，我们在输出层之前加入dropout。我们用一个<code>placeholder</code>来代表一个神经元的输出在dropout中保持不变的概率。这样我们可以在训练过程中启用dropout，在测试过程中关闭dropout。 TensorFlow的<code>tf.nn.dropout</code>操作除了可以屏蔽神经元的输出外，还会自动处理神经元输出值的scale。所以用dropout的时候可以不用考虑scale。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">keep_prob &#x3D; tf.placeholder(&quot;float&quot;)</span><br><span class="line">h_fc1_drop &#x3D; tf.nn.dropout(h_fc1, keep_prob)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="输出层"><a href="#输出层" class="headerlink" title="输出层"></a>输出层</h4><p>最后，我们添加一个softmax层，就像前面的单层softmax regression一样。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">W_fc2 &#x3D; weight_variable([1024, 10])</span><br><span class="line">b_fc2 &#x3D; bias_variable([10])</span><br><span class="line"></span><br><span class="line">y_conv&#x3D;tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="训练和评估模型"><a href="#训练和评估模型" class="headerlink" title="训练和评估模型"></a>训练和评估模型</h4><p>这个模型的效果如何呢？</p>
<p>为了进行训练和评估，我们使用与之前简单的单层SoftMax神经网络模型几乎相同的一套代码，只是我们会用更加复杂的ADAM优化器来做梯度最速下降，在<code>feed_dict</code>中加入额外的参数<code>keep_prob</code>来控制dropout比例。然后每100次迭代输出一次日志。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">cross_entropy &#x3D; -tf.reduce_sum(y_*tf.log(y_conv))</span><br><span class="line">train_step &#x3D; tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)</span><br><span class="line">correct_prediction &#x3D; tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))</span><br><span class="line">accuracy &#x3D; tf.reduce_mean(tf.cast(correct_prediction, &quot;float&quot;))</span><br><span class="line">sess.run(tf.initialize_all_variables())</span><br><span class="line">for i in range(20000):</span><br><span class="line">  batch &#x3D; mnist.train.next_batch(50)</span><br><span class="line">  if i%100 &#x3D;&#x3D; 0:</span><br><span class="line">    train_accuracy &#x3D; accuracy.eval(feed_dict&#x3D;&#123;</span><br><span class="line">        x:batch[0], y_: batch[1], keep_prob: 1.0&#125;)</span><br><span class="line">    print &quot;step %d, training accuracy %g&quot;%(i, train_accuracy)</span><br><span class="line">  train_step.run(feed_dict&#x3D;&#123;x: batch[0], y_: batch[1], keep_prob: 0.5&#125;)</span><br><span class="line"></span><br><span class="line">print &quot;test accuracy %g&quot;%accuracy.eval(feed_dict&#x3D;&#123;</span><br><span class="line">    x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0&#125;)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>以上代码，在最终测试集上的准确率大概是99.2%。</p>
<h1 id="公众号推荐"><a href="#公众号推荐" class="headerlink" title="公众号推荐"></a>公众号推荐</h1><p>推荐一波自己的公众号：五道口的程序狐</p>
<p>里面有一个聊天机器人，抚慰你的心灵</p>
<p><img src="https://upload-images.jianshu.io/upload_images/5060560-ac8a0f00b9f836b0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="mp"></p>
<p>如有需要，联系<code>contact@fhao.top</code></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://fengh16.github.io/2018/04/19/TensorFlow-%E5%AE%89%E8%A3%85%E4%B8%8E%E4%BD%BF%E7%94%A8/" data-id="ckqlzg1xe00piwkc4hqa6e6j3" data-title="TensorFlow-安装与使用" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2018/04/19/windows%E4%B8%8Bpip%E6%8D%A2%E6%BA%90/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          windows下pip换源
        
      </div>
    </a>
  
  
    <a href="/2018/04/19/xv6-boot%E9%83%A8%E5%88%86/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">xv6-boot部分</div>
    </a>
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%BD%BF%E7%94%A8%E2%80%94%E6%97%A5%E5%B8%B8%E7%B3%BB%E7%BB%9F/">使用—日常系统</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%BD%BF%E7%94%A8%E2%80%94%E6%9C%8D%E5%8A%A1%E5%99%A8/">使用—服务器</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%BD%BF%E7%94%A8%E2%80%94%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%BC%8F%E2%80%94%E5%A4%9A%E8%BF%9B%E7%A8%8B/">使用—编程模式—多进程</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%BD%BF%E7%94%A8%E2%80%94%E8%BD%AF%E4%BB%B6%E2%80%94Antlr4/">使用—软件—Antlr4</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%BD%BF%E7%94%A8%E2%80%94%E8%BD%AF%E4%BB%B6%E2%80%94Django/">使用—软件—Django</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%BD%BF%E7%94%A8%E2%80%94%E8%BD%AF%E4%BB%B6%E2%80%94Git/">使用—软件—Git</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%BD%BF%E7%94%A8%E2%80%94%E8%BD%AF%E4%BB%B6%E2%80%94RN/">使用—软件—RN</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%BD%BF%E7%94%A8%E2%80%94%E8%BD%AF%E4%BB%B6%E2%80%94Selenium/">使用—软件—Selenium</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%BD%BF%E7%94%A8%E2%80%94%E8%BD%AF%E4%BB%B6%E2%80%94TF/">使用—软件—TF</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%BD%BF%E7%94%A8%E2%80%94%E8%BD%AF%E4%BB%B6%E2%80%94docker/">使用—软件—docker</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%BD%BF%E7%94%A8%E2%80%94%E8%BD%AF%E4%BB%B6%E2%80%94%E5%85%B6%E4%BB%96/">使用—软件—其他</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%95%B0%E6%8D%AE%E9%9B%86/">数据集</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%94%9F%E6%B4%BB/">生活</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%A7%91%E7%A0%94/">科研</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%AE%97%E6%B3%95/">算法</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%B3%BB%E7%BB%9F%E4%BD%BF%E7%94%A8/">系统使用</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%AF%AD%E8%A8%80%E2%80%94C/">语言—C++</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%AF%AD%E8%A8%80%E2%80%94Haskell/">语言—Haskell</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%AF%AD%E8%A8%80%E2%80%94Java/">语言—Java</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%AF%AD%E8%A8%80%E2%80%94PHP/">语言—PHP</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%AF%AD%E8%A8%80%E2%80%94Python/">语言—Python</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%AF%AD%E8%A8%80%E2%80%94R%E8%AF%AD%E8%A8%80/">语言—R语言</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%AF%AD%E8%A8%80%E2%80%94%E6%B1%87%E7%BC%96/">语言—汇编</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%B5%84%E6%BA%90/">资源</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%BD%AF%E4%BB%B6%E4%BD%BF%E7%94%A8/">软件使用</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E9%87%91%E8%9E%8D/">金融</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E9%A2%86%E5%9F%9F%E2%80%94FTP%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/">领域—FTP计算机网络</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E9%A2%86%E5%9F%9F%E2%80%94Win32%E6%A1%8C%E9%9D%A2%E7%A8%8B%E5%BA%8F/">领域—Win32桌面程序</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E9%A2%86%E5%9F%9F%E2%80%94%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">领域—人工智能神经网络</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E9%A2%86%E5%9F%9F%E2%80%94%E5%89%8D%E7%AB%AF/">领域—前端</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E9%A2%86%E5%9F%9F%E2%80%94%E5%BE%AE%E4%BF%A1%E5%85%AC%E4%BC%97%E5%8F%B7/">领域—微信公众号</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E9%A2%86%E5%9F%9F%E2%80%94%E6%93%8D%E7%BB%9F%E2%80%94xv6/">领域—操统—xv6</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E9%A2%86%E5%9F%9F%E2%80%94%E6%95%B0%E5%AD%A6/">领域—数学</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E9%A2%86%E5%9F%9F%E2%80%94%E6%B5%8B%E8%AF%95/">领域—测试</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E9%A2%86%E5%9F%9F%E2%80%94%E7%A4%BE%E4%BA%A4%E7%BD%91%E7%BB%9C%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/">领域—社交网络推荐系统</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E9%A2%86%E5%9F%9F%E2%80%94%E8%BD%AF%E4%BB%B6%E5%B7%A5%E7%A8%8B/">领域—软件工程</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E9%A2%86%E5%9F%9F%E2%80%94%E9%BB%91%E5%AE%A2%E6%8A%80%E6%9C%AF/">领域—黑客技术</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/C/" rel="tag">C++</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CLion/" rel="tag">CLion</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/C%E8%AF%AD%E8%A8%80/" rel="tag">C语言</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/GNN/" rel="tag">GNN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Kaggle/" rel="tag">Kaggle</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/PyCharm/" rel="tag">PyCharm</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Python/" rel="tag">Python</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Windows/" rel="tag">Windows</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/tensorflow/" rel="tag">tensorflow</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%95%B0%E6%8D%AE%E9%9B%86/" rel="tag">数据集</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%9C%8D%E5%8A%A1%E5%99%A8/" rel="tag">服务器</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%BC%80%E5%8F%91/" rel="tag">服务器开发</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%A4%BE%E4%BA%A4%E7%BD%91%E7%BB%9C/" rel="tag">社交网络</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%A4%BE%E5%8C%BA%E5%88%92%E5%88%86/" rel="tag">社区划分</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" rel="tag">神经网络</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%AE%97%E6%B3%95/" rel="tag">算法</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%B3%BB%E7%BB%9F%E4%BD%BF%E7%94%A8/" rel="tag">系统使用</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%BD%AC%E8%BD%BD/" rel="tag">转载</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%BD%AF%E4%BB%B6%E4%BD%BF%E7%94%A8/" rel="tag">软件使用</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%BF%9C%E7%A8%8B/" rel="tag">远程</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/C/" style="font-size: 10px;">C++</a> <a href="/tags/CLion/" style="font-size: 10px;">CLion</a> <a href="/tags/C%E8%AF%AD%E8%A8%80/" style="font-size: 12.5px;">C语言</a> <a href="/tags/GNN/" style="font-size: 12.5px;">GNN</a> <a href="/tags/Kaggle/" style="font-size: 10px;">Kaggle</a> <a href="/tags/PyCharm/" style="font-size: 10px;">PyCharm</a> <a href="/tags/Python/" style="font-size: 17.5px;">Python</a> <a href="/tags/Windows/" style="font-size: 10px;">Windows</a> <a href="/tags/tensorflow/" style="font-size: 10px;">tensorflow</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E9%9B%86/" style="font-size: 12.5px;">数据集</a> <a href="/tags/%E6%9C%8D%E5%8A%A1%E5%99%A8/" style="font-size: 15px;">服务器</a> <a href="/tags/%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%BC%80%E5%8F%91/" style="font-size: 15px;">服务器开发</a> <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" style="font-size: 10px;">机器学习</a> <a href="/tags/%E7%A4%BE%E4%BA%A4%E7%BD%91%E7%BB%9C/" style="font-size: 12.5px;">社交网络</a> <a href="/tags/%E7%A4%BE%E5%8C%BA%E5%88%92%E5%88%86/" style="font-size: 12.5px;">社区划分</a> <a href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" style="font-size: 20px;">神经网络</a> <a href="/tags/%E7%AE%97%E6%B3%95/" style="font-size: 20px;">算法</a> <a href="/tags/%E7%B3%BB%E7%BB%9F%E4%BD%BF%E7%94%A8/" style="font-size: 10px;">系统使用</a> <a href="/tags/%E8%BD%AC%E8%BD%BD/" style="font-size: 17.5px;">转载</a> <a href="/tags/%E8%BD%AF%E4%BB%B6%E4%BD%BF%E7%94%A8/" style="font-size: 20px;">软件使用</a> <a href="/tags/%E8%BF%9C%E7%A8%8B/" style="font-size: 15px;">远程</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/07/">July 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/06/">June 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/05/">May 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/04/">April 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/03/">March 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/02/">February 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/01/">January 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/12/">December 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/11/">November 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/10/">October 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/09/">September 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/08/">August 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/07/">July 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/06/">June 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/05/">May 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/04/">April 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/03/">March 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/02/">February 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/01/">January 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/12/">December 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/11/">November 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/10/">October 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/09/">September 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/08/">August 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/07/">July 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/06/">June 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/05/">May 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/04/">April 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/03/">March 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/01/">January 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/12/">December 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/11/">November 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/10/">October 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/09/">September 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/08/">August 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/07/">July 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/06/">June 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/05/">May 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/04/">April 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/03/">March 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/1970/01/">January 1970</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2021/07/01/Machine%20Learning%20in%20Action/">Machine Learning in Action总结</a>
          </li>
        
          <li>
            <a href="/2021/06/29/Windows-%E4%BF%AE%E6%94%B9%E8%BF%9C%E7%A8%8B%E6%A1%8C%E9%9D%A2/">Windows-修改远程桌面</a>
          </li>
        
          <li>
            <a href="/2021/06/28/switch%E6%A8%A1%E6%8B%9F%E5%99%A8/">switch模拟器</a>
          </li>
        
          <li>
            <a href="/2021/06/27/latex-label%E4%B8%8D%E5%AF%B9/">latex-label不对</a>
          </li>
        
          <li>
            <a href="/2021/06/27/latex-%E6%8E%92%E7%89%88/">latex-排版</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2021 Henry Fox<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.4.1.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>